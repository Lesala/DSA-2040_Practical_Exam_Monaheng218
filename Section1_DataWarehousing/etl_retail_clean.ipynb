{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44283dba",
   "metadata": {},
   "source": [
    "# DSA 2040 Practical Exam - Section 1, Task 2\n",
    "## ETL Process Implementation for Retail Data Warehouse\n",
    "\n",
    "**Student:** Monaheng218  \n",
    "**Date:** August 13, 2025  \n",
    "**Total Marks:** 20\n",
    "\n",
    "### Task Requirements:\n",
    "1. Generate synthetic retail dataset (~1000 rows)\n",
    "2. Extract data with validation and cleaning\n",
    "3. Transform data for star schema design\n",
    "4. Load data into SQLite database\n",
    "5. Verify data integrity and provide summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d1a8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports and setup complete\n",
      "Python version: 2.0.3 (pandas)\n",
      "NumPy version: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducible results\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('etl_process.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Imports and setup complete\")\n",
    "print(f\"Python version: {pd.__version__} (pandas)\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f6bb948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ETL functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ETL FUNCTIONS DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_synthetic_retail_data(num_rows=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic retail data similar to UCI Online Retail dataset structure.\n",
    "    \n",
    "    Columns: InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country\n",
    "    \n",
    "    Args:\n",
    "        num_rows (int): Number of transactions to generate (default: 1000)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Generated synthetic retail data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating {num_rows} rows of synthetic retail data...\")\n",
    "    \n",
    "    # Define data generation parameters\n",
    "    countries = ['United Kingdom', 'France', 'Australia', 'Netherlands', 'Germany', \n",
    "                'Norway', 'EIRE', 'Switzerland', 'Spain', 'Poland', 'Portugal', \n",
    "                'Italy', 'Belgium', 'Lithuania', 'Japan', 'Iceland']\n",
    "    \n",
    "    product_categories = ['HOME', 'GARDEN', 'GIFT', 'OFFICE', 'KITCHEN', 'DECOR', 'TOY']\n",
    "    \n",
    "    # Generate base data\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "    \n",
    "    data = []\n",
    "    invoice_counter = 536365  # Starting invoice number (UCI format)\n",
    "    customer_ids = list(range(12346, 12346 + 200))  # Pool of customer IDs\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        # Generate invoice details\n",
    "        if i % random.randint(1, 5) == 0:  # New invoice every 1-5 items\n",
    "            invoice_counter += 1\n",
    "            current_invoice = str(invoice_counter)\n",
    "            current_customer = random.choice(customer_ids)\n",
    "            current_country = random.choice(countries)\n",
    "            invoice_date = fake.date_time_between(start_date=start_date, end_date=end_date)\n",
    "        \n",
    "        # Generate product details\n",
    "        category = random.choice(product_categories)\n",
    "        stock_code = f\"{category[:2]}{random.randint(10000, 99999)}\"\n",
    "        description = f\"{category.title()} {fake.word().title()} {fake.word().title()}\"\n",
    "        \n",
    "        # Generate quantities and prices\n",
    "        quantity = random.randint(1, 50)\n",
    "        unit_price = round(random.uniform(0.5, 25.0), 2)\n",
    "        \n",
    "        # Add some cancelled orders (negative quantities)\n",
    "        if random.random() < 0.05:  # 5% cancelled orders\n",
    "            quantity = -quantity\n",
    "            current_invoice = f\"C{current_invoice}\"  # Cancelled invoice prefix\n",
    "        \n",
    "        data.append({\n",
    "            'InvoiceNo': current_invoice,\n",
    "            'StockCode': stock_code,\n",
    "            'Description': description,\n",
    "            'Quantity': quantity,\n",
    "            'InvoiceDate': invoice_date,\n",
    "            'UnitPrice': unit_price,\n",
    "            'CustomerID': current_customer,\n",
    "            'Country': current_country\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some missing values to simulate real-world data\n",
    "    missing_indices = random.sample(range(len(df)), int(len(df) * 0.02))  # 2% missing\n",
    "    df.loc[missing_indices, 'Description'] = None\n",
    "    \n",
    "    logger.info(f\"Generated dataset shape: {df.shape}\")\n",
    "    logger.info(f\"Date range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}\")\n",
    "    logger.info(f\"Unique customers: {df['CustomerID'].nunique()}\")\n",
    "    logger.info(f\"Unique products: {df['StockCode'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_data(raw_data):\n",
    "    \"\"\"\n",
    "    Extract and validate raw data.\n",
    "    \n",
    "    Args:\n",
    "        raw_data (pd.DataFrame): Raw retail data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Validated extracted data\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data extraction phase...\")\n",
    "    \n",
    "    extracted_data = raw_data.copy()\n",
    "    original_rows = len(extracted_data)\n",
    "    \n",
    "    # Data type conversion\n",
    "    extracted_data['InvoiceDate'] = pd.to_datetime(extracted_data['InvoiceDate'])\n",
    "    extracted_data['Quantity'] = pd.to_numeric(extracted_data['Quantity'], errors='coerce')\n",
    "    extracted_data['UnitPrice'] = pd.to_numeric(extracted_data['UnitPrice'], errors='coerce')\n",
    "    extracted_data['CustomerID'] = pd.to_numeric(extracted_data['CustomerID'], errors='coerce')\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_before = extracted_data.isnull().sum().sum()\n",
    "    extracted_data['Description'] = extracted_data['Description'].fillna('Unknown Product')\n",
    "    missing_after = extracted_data.isnull().sum().sum()\n",
    "    \n",
    "    # Remove rows with missing critical fields\n",
    "    critical_fields = ['InvoiceNo', 'StockCode', 'Quantity', 'UnitPrice', 'CustomerID']\n",
    "    extracted_data = extracted_data.dropna(subset=critical_fields)\n",
    "    \n",
    "    final_rows = len(extracted_data)\n",
    "    \n",
    "    logger.info(f\"Extraction complete:\")\n",
    "    logger.info(f\"  - Original rows: {original_rows}\")\n",
    "    logger.info(f\"  - Missing values filled: {missing_before - missing_after}\")\n",
    "    logger.info(f\"  - Final rows: {final_rows}\")\n",
    "    logger.info(f\"  - Rows removed: {original_rows - final_rows}\")\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "def transform_data(extracted_data):\n",
    "    \"\"\"\n",
    "    Transform data for star schema dimensional model.\n",
    "    \n",
    "    Args:\n",
    "        extracted_data (pd.DataFrame): Extracted data\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (sales_fact, customer_dim, product_dim, time_dim)\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data transformation phase...\")\n",
    "    \n",
    "    df = extracted_data.copy()\n",
    "    \n",
    "    # Calculate total sales\n",
    "    df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "    \n",
    "    # Filter for last year data only (2023)\n",
    "    df = df[df['InvoiceDate'].dt.year == 2023]\n",
    "    \n",
    "    # Remove outliers (extremely high quantities or prices)\n",
    "    df = df[(df['Quantity'].abs() <= 100) & (df['UnitPrice'] <= 100)]\n",
    "    \n",
    "    # Create Customer Dimension\n",
    "    customer_dim = df[['CustomerID', 'Country']].drop_duplicates().reset_index(drop=True)\n",
    "    customer_dim['CustomerKey'] = range(1, len(customer_dim) + 1)\n",
    "    customer_dim = customer_dim[['CustomerKey', 'CustomerID', 'Country']]\n",
    "    \n",
    "    # Create Product Dimension\n",
    "    product_dim = df[['StockCode', 'Description']].drop_duplicates().reset_index(drop=True)\n",
    "    product_dim['ProductKey'] = range(1, len(product_dim) + 1)\n",
    "    product_dim['Category'] = product_dim['StockCode'].str[:2]  # Extract category from stock code\n",
    "    product_dim = product_dim[['ProductKey', 'StockCode', 'Description', 'Category']]\n",
    "    \n",
    "    # Create Time Dimension\n",
    "    time_data = []\n",
    "    for date in df['InvoiceDate'].dt.date.unique():\n",
    "        dt = pd.to_datetime(date)\n",
    "        time_data.append({\n",
    "            'Date': date,\n",
    "            'Year': dt.year,\n",
    "            'Month': dt.month,\n",
    "            'Day': dt.day,\n",
    "            'Quarter': dt.quarter,\n",
    "            'DayOfWeek': dt.dayofweek + 1,\n",
    "            'MonthName': dt.strftime('%B'),\n",
    "            'DayName': dt.strftime('%A')\n",
    "        })\n",
    "    \n",
    "    time_dim = pd.DataFrame(time_data).drop_duplicates().reset_index(drop=True)\n",
    "    time_dim['TimeKey'] = range(1, len(time_dim) + 1)\n",
    "    time_dim = time_dim[['TimeKey', 'Date', 'Year', 'Month', 'Day', 'Quarter', \n",
    "                        'DayOfWeek', 'MonthName', 'DayName']]\n",
    "    \n",
    "    # Create Sales Fact table\n",
    "    sales_fact = df.merge(customer_dim[['CustomerKey', 'CustomerID']], on='CustomerID')\n",
    "    sales_fact = sales_fact.merge(product_dim[['ProductKey', 'StockCode']], on='StockCode')\n",
    "    sales_fact = sales_fact.merge(time_dim[['TimeKey', 'Date']], \n",
    "                                 left_on=sales_fact['InvoiceDate'].dt.date, \n",
    "                                 right_on='Date')\n",
    "    \n",
    "    sales_fact = sales_fact[['InvoiceNo', 'CustomerKey', 'ProductKey', 'TimeKey', \n",
    "                           'Quantity', 'UnitPrice', 'TotalSales']]\n",
    "    sales_fact['SalesKey'] = range(1, len(sales_fact) + 1)\n",
    "    sales_fact = sales_fact[['SalesKey', 'InvoiceNo', 'CustomerKey', 'ProductKey', \n",
    "                           'TimeKey', 'Quantity', 'UnitPrice', 'TotalSales']]\n",
    "    \n",
    "    logger.info(f\"Transformation complete:\")\n",
    "    logger.info(f\"  - Sales Fact records: {len(sales_fact)}\")\n",
    "    logger.info(f\"  - Customer Dimension records: {len(customer_dim)}\")\n",
    "    logger.info(f\"  - Product Dimension records: {len(product_dim)}\")\n",
    "    logger.info(f\"  - Time Dimension records: {len(time_dim)}\")\n",
    "    \n",
    "    return sales_fact, customer_dim, product_dim, time_dim\n",
    "\n",
    "def load_data_to_warehouse(sales_fact, customer_dim, product_dim, time_dim, db_path='retail_dw.db'):\n",
    "    \"\"\"\n",
    "    Load transformed data into SQLite data warehouse.\n",
    "    \n",
    "    Args:\n",
    "        sales_fact (pd.DataFrame): Sales fact table\n",
    "        customer_dim (pd.DataFrame): Customer dimension\n",
    "        product_dim (pd.DataFrame): Product dimension\n",
    "        time_dim (pd.DataFrame): Time dimension\n",
    "        db_path (str): Database file path\n",
    "        \n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting data loading to {db_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Remove existing database\n",
    "        if os.path.exists(db_path):\n",
    "            os.remove(db_path)\n",
    "            logger.info(f\"Removed existing database: {db_path}\")\n",
    "        \n",
    "        # Create connection\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Read and execute schema - try current directory first\n",
    "        schema_path = 'warehouse_schema.sql'\n",
    "        if not os.path.exists(schema_path):\n",
    "            # Try parent directory\n",
    "            schema_path = '../warehouse_schema.sql'\n",
    "        \n",
    "        if os.path.exists(schema_path):\n",
    "            with open(schema_path, 'r') as f:\n",
    "                schema_sql = f.read()\n",
    "            conn.executescript(schema_sql)\n",
    "            logger.info(f\"Database schema created successfully from {schema_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"Schema file not found, creating basic tables\")\n",
    "            # Create basic schema if file doesn't exist\n",
    "            basic_schema = \"\"\"\n",
    "            CREATE TABLE CustomerDim (\n",
    "                CustomerKey INTEGER PRIMARY KEY,\n",
    "                CustomerID INTEGER,\n",
    "                Country TEXT\n",
    "            );\n",
    "            \n",
    "            CREATE TABLE ProductDim (\n",
    "                ProductKey INTEGER PRIMARY KEY,\n",
    "                StockCode TEXT,\n",
    "                Description TEXT,\n",
    "                Category TEXT\n",
    "            );\n",
    "            \n",
    "            CREATE TABLE TimeDim (\n",
    "                TimeKey INTEGER PRIMARY KEY,\n",
    "                Date DATE,\n",
    "                Year INTEGER,\n",
    "                Month INTEGER,\n",
    "                Day INTEGER,\n",
    "                Quarter INTEGER,\n",
    "                DayOfWeek INTEGER,\n",
    "                MonthName TEXT,\n",
    "                DayName TEXT\n",
    "            );\n",
    "            \n",
    "            CREATE TABLE SalesFact (\n",
    "                SalesKey INTEGER PRIMARY KEY,\n",
    "                InvoiceNo TEXT,\n",
    "                CustomerKey INTEGER,\n",
    "                ProductKey INTEGER,\n",
    "                TimeKey INTEGER,\n",
    "                Quantity INTEGER,\n",
    "                UnitPrice DECIMAL(10,2),\n",
    "                TotalSales DECIMAL(10,2),\n",
    "                FOREIGN KEY (CustomerKey) REFERENCES CustomerDim(CustomerKey),\n",
    "                FOREIGN KEY (ProductKey) REFERENCES ProductDim(ProductKey),\n",
    "                FOREIGN KEY (TimeKey) REFERENCES TimeDim(TimeKey)\n",
    "            );\n",
    "            \"\"\"\n",
    "            conn.executescript(basic_schema)\n",
    "        \n",
    "        # Load dimension tables first\n",
    "        customer_dim.to_sql('CustomerDim', conn, if_exists='append', index=False)\n",
    "        product_dim.to_sql('ProductDim', conn, if_exists='append', index=False)\n",
    "        time_dim.to_sql('TimeDim', conn, if_exists='append', index=False)\n",
    "        \n",
    "        # Load fact table\n",
    "        sales_fact.to_sql('SalesFact', conn, if_exists='append', index=False)\n",
    "        \n",
    "        # Verify data integrity\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Check row counts\n",
    "        tables = ['CustomerDim', 'ProductDim', 'TimeDim', 'SalesFact']\n",
    "        for table in tables:\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            logger.info(f\"  - {table}: {count} rows loaded\")\n",
    "        \n",
    "        # Verify foreign key integrity\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT COUNT(*) FROM SalesFact sf\n",
    "            LEFT JOIN CustomerDim cd ON sf.CustomerKey = cd.CustomerKey\n",
    "            LEFT JOIN ProductDim pd ON sf.ProductKey = pd.ProductKey\n",
    "            LEFT JOIN TimeDim td ON sf.TimeKey = td.TimeKey\n",
    "            WHERE cd.CustomerKey IS NULL OR pd.ProductKey IS NULL OR td.TimeKey IS NULL\n",
    "        \"\"\")\n",
    "        orphaned_records = cursor.fetchone()[0]\n",
    "        \n",
    "        if orphaned_records > 0:\n",
    "            logger.warning(f\"Found {orphaned_records} orphaned records in SalesFact\")\n",
    "        else:\n",
    "            logger.info(\"‚úÖ All foreign key constraints verified\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        logger.info(f\"‚úÖ Data successfully loaded to {db_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Load phase failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def perform_full_etl(num_rows=1000, db_path='retail_dw.db'):\n",
    "    \"\"\"\n",
    "    Perform the complete ETL process and log the number of rows processed at each stage.\n",
    "    \n",
    "    Args:\n",
    "        num_rows (int): Number of rows to generate\n",
    "        db_path (str): Database file path\n",
    "        \n",
    "    Returns:\n",
    "        dict: ETL process summary with row counts\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"STARTING COMPLETE ETL PROCESS\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Generate/Extract\n",
    "        logger.info(\"STAGE 1: DATA GENERATION & EXTRACTION\")\n",
    "        raw_data = generate_synthetic_retail_data(num_rows)\n",
    "        extracted_data = extract_data(raw_data)\n",
    "        extract_rows = len(extracted_data)\n",
    "        \n",
    "        # Stage 2: Transform\n",
    "        logger.info(\"STAGE 2: DATA TRANSFORMATION\")\n",
    "        sales_fact, customer_dim, product_dim, time_dim = transform_data(extracted_data)\n",
    "        transform_rows = len(sales_fact)\n",
    "        \n",
    "        # Stage 3: Load\n",
    "        logger.info(\"STAGE 3: DATA LOADING\")\n",
    "        load_success = load_data_to_warehouse(sales_fact, customer_dim, product_dim, time_dim, db_path)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # ETL Summary\n",
    "        summary = {\n",
    "            'status': 'SUCCESS' if load_success else 'FAILED',\n",
    "            'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'end_time': end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'duration_seconds': duration,\n",
    "            'rows_generated': num_rows,\n",
    "            'rows_extracted': extract_rows,\n",
    "            'rows_transformed': transform_rows,\n",
    "            'fact_table_rows': len(sales_fact),\n",
    "            'customer_dimension_rows': len(customer_dim),\n",
    "            'product_dimension_rows': len(product_dim),\n",
    "            'time_dimension_rows': len(time_dim),\n",
    "            'database_file': db_path\n",
    "        }\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"ETL PROCESS COMPLETED\")\n",
    "        logger.info(f\"Status: {summary['status']}\")\n",
    "        logger.info(f\"Total Duration: {duration:.2f} seconds\")\n",
    "        logger.info(f\"Rows Processed: {num_rows} ‚Üí {extract_rows} ‚Üí {transform_rows}\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ETL process failed: {str(e)}\")\n",
    "        return {'status': 'FAILED', 'error': str(e)}\n",
    "\n",
    "print(\"‚úÖ ETL functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c8c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 16:04:57,887 - INFO - ============================================================\n",
      "2025-08-13 16:04:57,887 - INFO - STARTING COMPLETE ETL PROCESS\n",
      "2025-08-13 16:04:57,887 - INFO - ============================================================\n",
      "2025-08-13 16:04:57,903 - INFO - STAGE 1: DATA GENERATION & EXTRACTION\n",
      "2025-08-13 16:04:57,903 - INFO - Generating 1000 rows of synthetic retail data...\n",
      "2025-08-13 16:04:58,048 - INFO - Generated dataset shape: (1000, 8)\n",
      "2025-08-13 16:04:58,048 - INFO - Date range: 2023-01-04 04:13:08 to 2023-12-30 18:23:29\n",
      "2025-08-13 16:04:58,063 - INFO - Unique customers: 177\n",
      "2025-08-13 16:04:58,063 - INFO - Unique products: 999\n",
      "2025-08-13 16:04:58,063 - INFO - Starting data extraction phase...\n",
      "2025-08-13 16:04:58,079 - INFO - Extraction complete:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPLETE ETL PROCESS EXECUTION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 16:04:58,079 - INFO -   - Original rows: 1000\n",
      "2025-08-13 16:04:58,079 - INFO -   - Missing values filled: 20\n",
      "2025-08-13 16:04:58,079 - INFO -   - Final rows: 1000\n",
      "2025-08-13 16:04:58,079 - INFO -   - Rows removed: 0\n",
      "2025-08-13 16:04:58,079 - INFO - STAGE 2: DATA TRANSFORMATION\n",
      "2025-08-13 16:04:58,095 - INFO - Starting data transformation phase...\n",
      "2025-08-13 16:04:58,171 - INFO - Transformation complete:\n",
      "2025-08-13 16:04:58,187 - INFO -   - Sales Fact records: 3101\n",
      "2025-08-13 16:04:58,187 - INFO -   - Customer Dimension records: 419\n",
      "2025-08-13 16:04:58,187 - INFO -   - Product Dimension records: 1000\n",
      "2025-08-13 16:04:58,187 - INFO -   - Time Dimension records: 245\n",
      "2025-08-13 16:04:58,187 - INFO - STAGE 3: DATA LOADING\n",
      "2025-08-13 16:04:58,187 - INFO - Starting data loading to retail_dw.db...\n",
      "2025-08-13 16:04:58,187 - INFO - Removed existing database: retail_dw.db\n",
      "2025-08-13 16:04:58,303 - INFO - Database schema created successfully from warehouse_schema.sql\n",
      "2025-08-13 16:04:58,423 - INFO -   - CustomerDim: 419 rows loaded\n",
      "2025-08-13 16:04:58,424 - INFO -   - ProductDim: 1000 rows loaded\n",
      "2025-08-13 16:04:58,424 - INFO -   - TimeDim: 245 rows loaded\n",
      "2025-08-13 16:04:58,424 - INFO -   - SalesFact: 3101 rows loaded\n",
      "2025-08-13 16:04:58,442 - INFO - ‚úÖ All foreign key constraints verified\n",
      "2025-08-13 16:04:58,442 - INFO - ‚úÖ Data successfully loaded to retail_dw.db\n",
      "2025-08-13 16:04:58,442 - INFO - ============================================================\n",
      "2025-08-13 16:04:58,442 - INFO - ETL PROCESS COMPLETED\n",
      "2025-08-13 16:04:58,442 - INFO - Status: SUCCESS\n",
      "2025-08-13 16:04:58,456 - INFO - Total Duration: 0.56 seconds\n",
      "2025-08-13 16:04:58,458 - INFO - Rows Processed: 1000 ‚Üí 1000 ‚Üí 3101\n",
      "2025-08-13 16:04:58,458 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ETL PROCESS SUMMARY\n",
      "------------------------------\n",
      "Status: SUCCESS\n",
      "Start Time: 2025-08-13 16:04:57\n",
      "End Time: 2025-08-13 16:04:58\n",
      "Duration Seconds: 0.555013\n",
      "Rows Generated: 1000\n",
      "Rows Extracted: 1000\n",
      "Rows Transformed: 3101\n",
      "Fact Table Rows: 3101\n",
      "Customer Dimension Rows: 419\n",
      "Product Dimension Rows: 1000\n",
      "Time Dimension Rows: 245\n",
      "Database File: retail_dw.db\n",
      "\n",
      "‚úÖ ETL Process completed successfully!\n",
      "Database 'retail_dw.db' is ready for OLAP queries.\n",
      "\n",
      "üìä Quick Database Verification:\n",
      "  - Total Sales Revenue: $881802.21\n",
      "  - Total Transactions: 3101\n",
      "  - Unique Customers: 419\n",
      "  - Unique Products: 1000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTE COMPLETE ETL PROCESS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE ETL PROCESS EXECUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Execute ETL process with 1000 rows\n",
    "etl_summary = perform_full_etl(1000, 'retail_dw.db')\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\nETL PROCESS SUMMARY\")\n",
    "print(\"-\" * 30)\n",
    "for key, value in etl_summary.items():\n",
    "    if key != 'error':\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Verify final database state\n",
    "if etl_summary['status'] == 'SUCCESS':\n",
    "    print(\"\\n‚úÖ ETL Process completed successfully!\")\n",
    "    print(\"Database 'retail_dw.db' is ready for OLAP queries.\")\n",
    "    \n",
    "    # Quick verification query\n",
    "    import sqlite3\n",
    "    conn = sqlite3.connect('retail_dw.db')\n",
    "    \n",
    "    print(\"\\nüìä Quick Database Verification:\")\n",
    "    verification_query = \"\"\"\n",
    "    SELECT \n",
    "        'Total Sales Revenue' as Metric,\n",
    "        printf('$%.2f', SUM(TotalSales)) as Value\n",
    "    FROM SalesFact\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Total Transactions' as Metric,\n",
    "        COUNT(*) as Value\n",
    "    FROM SalesFact\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Unique Customers' as Metric,\n",
    "        COUNT(DISTINCT CustomerKey) as Value\n",
    "    FROM SalesFact\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Unique Products' as Metric,\n",
    "        COUNT(*) as Value\n",
    "    FROM ProductDim\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.read_sql_query(verification_query, conn)\n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"  - {row['Metric']}: {row['Value']}\")\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(\"\\n‚ùå ETL Process failed. Check logs for details.\")\n",
    "    if 'error' in etl_summary:\n",
    "        print(f\"Error: {etl_summary['error']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
