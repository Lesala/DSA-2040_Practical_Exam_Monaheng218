{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f256f3a6",
   "metadata": {},
   "source": [
    "# DSA 2040 Practical Exam - Section 2, Task 4\n",
    "## Association Rules - Market Basket Analysis\n",
    "\n",
    "**Student:** Monaheng218  \n",
    "**Date:** August 13, 2025  \n",
    "**Total Marks:** 10\n",
    "\n",
    "### Task Requirements:\n",
    "1. Perform market basket analysis using association rules\n",
    "2. Find frequent itemsets and strong association rules\n",
    "3. Analyze support, confidence, and lift metrics\n",
    "4. Identify product associations and cross-selling opportunities\n",
    "5. Provide business recommendations for product placement and marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44003ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Association rules libraries imported successfully\")\n",
    "print(\"üõí Ready for market basket analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD AND PREPARE TRANSACTION DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: LOADING AND PREPARING TRANSACTION DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Connect to the data warehouse\n",
    "try:\n",
    "    conn = sqlite3.connect('../Section1_DataWarehousing/retail_dw.db')\n",
    "    print(\"‚úÖ Connected to data warehouse\")\n",
    "except:\n",
    "    # Fallback to preprocessed data if warehouse not available\n",
    "    print(\"‚ö†Ô∏è Data warehouse not found, creating sample transaction data...\")\n",
    "    \n",
    "    # Create sample transaction data for demonstration\n",
    "    np.random.seed(42)\n",
    "    products = [\n",
    "        'Electronics_Laptop', 'Electronics_Phone', 'Electronics_Tablet',\n",
    "        'Clothing_Shirt', 'Clothing_Pants', 'Clothing_Shoes',\n",
    "        'Home_Furniture', 'Home_Decor', 'Home_Kitchen',\n",
    "        'Books_Fiction', 'Books_NonFiction', 'Books_Technical',\n",
    "        'Sports_Equipment', 'Sports_Clothing', 'Sports_Accessories'\n",
    "    ]\n",
    "    \n",
    "    transactions = []\n",
    "    for i in range(2000):\n",
    "        # Create realistic shopping baskets\n",
    "        basket_size = np.random.choice([1, 2, 3, 4, 5], p=[0.3, 0.3, 0.2, 0.15, 0.05])\n",
    "        \n",
    "        # Create category preferences\n",
    "        if np.random.random() < 0.3:  # Electronics preference\n",
    "            category_products = [p for p in products if p.startswith('Electronics')]\n",
    "            other_products = [p for p in products if not p.startswith('Electronics')]\n",
    "            basket = np.random.choice(category_products, min(basket_size, len(category_products)), replace=False).tolist()\n",
    "            if len(basket) < basket_size:\n",
    "                additional = np.random.choice(other_products, basket_size - len(basket), replace=False)\n",
    "                basket.extend(additional)\n",
    "        elif np.random.random() < 0.3:  # Clothing preference\n",
    "            category_products = [p for p in products if p.startswith('Clothing')]\n",
    "            other_products = [p for p in products if not p.startswith('Clothing')]\n",
    "            basket = np.random.choice(category_products, min(basket_size, len(category_products)), replace=False).tolist()\n",
    "            if len(basket) < basket_size:\n",
    "                additional = np.random.choice(other_products, basket_size - len(basket), replace=False)\n",
    "                basket.extend(additional)\n",
    "        else:  # Random basket\n",
    "            basket = np.random.choice(products, basket_size, replace=False).tolist()\n",
    "        \n",
    "        transactions.append(basket)\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(transactions)} sample transactions\")\n",
    "    conn = None\n",
    "\n",
    "# Load transaction data from data warehouse if available\n",
    "if conn:\n",
    "    # Extract transaction data with product information\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        sf.SaleID,\n",
    "        sf.CustomerID,\n",
    "        pd.Category || '_' || pd.ProductName as Product\n",
    "    FROM SalesFact sf\n",
    "    JOIN ProductDim pd ON sf.ProductID = pd.ProductID\n",
    "    ORDER BY sf.SaleID, sf.CustomerID\n",
    "    \"\"\"\n",
    "    \n",
    "    transaction_data = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"üìä Loaded {len(transaction_data)} transaction records\")\n",
    "    print(f\"   Unique transactions: {transaction_data['SaleID'].nunique()}\")\n",
    "    print(f\"   Unique customers: {transaction_data['CustomerID'].nunique()}\")\n",
    "    print(f\"   Unique products: {transaction_data['Product'].nunique()}\")\n",
    "    \n",
    "    # Group by transaction to create baskets\n",
    "    transactions = transaction_data.groupby('SaleID')['Product'].apply(list).tolist()\n",
    "    \n",
    "    print(f\"\\nüõí Transaction Statistics:\")\n",
    "    basket_sizes = [len(basket) for basket in transactions]\n",
    "    print(f\"   Average basket size: {np.mean(basket_sizes):.2f}\")\n",
    "    print(f\"   Median basket size: {np.median(basket_sizes):.0f}\")\n",
    "    print(f\"   Max basket size: {max(basket_sizes)}\")\n",
    "    print(f\"   Min basket size: {min(basket_sizes)}\")\n",
    "\n",
    "# Display sample transactions\n",
    "print(f\"\\nüìã Sample Transactions:\")\n",
    "for i, transaction in enumerate(transactions[:5]):\n",
    "    print(f\"   Transaction {i+1}: {transaction}\")\n",
    "\n",
    "# Analyze product frequency\n",
    "all_products = [item for transaction in transactions for item in transaction]\n",
    "product_frequency = Counter(all_products)\n",
    "top_products = product_frequency.most_common(10)\n",
    "\n",
    "print(f\"\\nüìä Top 10 Most Frequent Products:\")\n",
    "for product, count in top_products:\n",
    "    percentage = (count / len(all_products)) * 100\n",
    "    print(f\"   {product}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize basket size distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "basket_sizes = [len(basket) for basket in transactions]\n",
    "plt.hist(basket_sizes, bins=range(1, max(basket_sizes)+2), alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Basket Sizes', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Number of Items in Basket')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "top_products_df = pd.DataFrame(top_products, columns=['Product', 'Frequency'])\n",
    "plt.barh(range(len(top_products_df)), top_products_df['Frequency'], alpha=0.8)\n",
    "plt.yticks(range(len(top_products_df)), top_products_df['Product'])\n",
    "plt.title('Top 10 Most Frequent Products', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Frequency')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Transaction data loaded and analyzed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE BINARY TRANSACTION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: CREATING BINARY TRANSACTION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use TransactionEncoder to convert transactions to binary matrix\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "transaction_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(f\"üìä Binary transaction matrix created:\")\n",
    "print(f\"   Shape: {transaction_df.shape}\")\n",
    "print(f\"   Transactions: {transaction_df.shape[0]}\")\n",
    "print(f\"   Unique products: {transaction_df.shape[1]}\")\n",
    "\n",
    "# Calculate product support (frequency)\n",
    "product_support = transaction_df.mean().sort_values(ascending=False)\n",
    "print(f\"\\nüìà Product Support (Top 10):\")\n",
    "for product, support in product_support.head(10).items():\n",
    "    print(f\"   {product}: {support:.3f} ({support*100:.1f}%)\")\n",
    "\n",
    "# Display first few rows of the matrix\n",
    "print(f\"\\nüìã Sample of Binary Transaction Matrix:\")\n",
    "print(transaction_df.head().to_string())\n",
    "\n",
    "# Analyze sparsity\n",
    "total_elements = transaction_df.shape[0] * transaction_df.shape[1]\n",
    "non_zero_elements = transaction_df.sum().sum()\n",
    "sparsity = 1 - (non_zero_elements / total_elements)\n",
    "\n",
    "print(f\"\\nüìä Matrix Statistics:\")\n",
    "print(f\"   Total elements: {total_elements:,}\")\n",
    "print(f\"   Non-zero elements: {non_zero_elements:,}\")\n",
    "print(f\"   Sparsity: {sparsity:.3f} ({sparsity*100:.1f}%)\")\n",
    "print(f\"   Average items per transaction: {non_zero_elements/transaction_df.shape[0]:.2f}\")\n",
    "\n",
    "# Visualize product support distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(product_support.values, bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Product Support', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Support (Frequency)')\n",
    "plt.ylabel('Number of Products')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(product_support)+1), sorted(product_support.values, reverse=True), \n",
    "         marker='o', linewidth=2, markersize=4)\n",
    "plt.title('Product Support Ranking (Pareto Analysis)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Product Rank')\n",
    "plt.ylabel('Support')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Binary transaction matrix created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE FREQUENT ITEMSETS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: GENERATING FREQUENT ITEMSETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set minimum support threshold\n",
    "min_support = 0.01  # 1% minimum support\n",
    "print(f\"üéØ Minimum support threshold: {min_support} ({min_support*100}%)\")\n",
    "\n",
    "# Generate frequent itemsets using Apriori algorithm\n",
    "print(\"\\nüîç Running Apriori algorithm...\")\n",
    "frequent_itemsets = apriori(transaction_df, min_support=min_support, use_colnames=True, verbose=1)\n",
    "\n",
    "if len(frequent_itemsets) == 0:\n",
    "    print(\"‚ö†Ô∏è No frequent itemsets found with current threshold. Reducing threshold...\")\n",
    "    min_support = 0.005  # Reduce to 0.5%\n",
    "    frequent_itemsets = apriori(transaction_df, min_support=min_support, use_colnames=True, verbose=1)\n",
    "    \n",
    "    if len(frequent_itemsets) == 0:\n",
    "        print(\"‚ö†Ô∏è Still no frequent itemsets. Using very low threshold...\")\n",
    "        min_support = 0.001  # Reduce to 0.1%\n",
    "        frequent_itemsets = apriori(transaction_df, min_support=min_support, use_colnames=True, verbose=1)\n",
    "\n",
    "print(f\"\\nüìä Frequent Itemsets Analysis:\")\n",
    "print(f\"   Total frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "print(f\"   Final minimum support used: {min_support} ({min_support*100}%)\")\n",
    "\n",
    "if len(frequent_itemsets) > 0:\n",
    "    # Add itemset length column\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    \n",
    "    # Analyze itemsets by length\n",
    "    itemset_length_counts = frequent_itemsets['length'].value_counts().sort_index()\n",
    "    print(f\"\\nüìä Itemsets by Length:\")\n",
    "    for length, count in itemset_length_counts.items():\n",
    "        print(f\"   {length}-itemsets: {count}\")\n",
    "    \n",
    "    # Display top frequent itemsets\n",
    "    print(f\"\\nüèÜ Top 20 Frequent Itemsets:\")\n",
    "    top_itemsets = frequent_itemsets.nlargest(20, 'support')\n",
    "    for idx, row in top_itemsets.iterrows():\n",
    "        itemset_str = ', '.join(list(row['itemsets']))\n",
    "        print(f\"   {itemset_str}: {row['support']:.3f} ({row['support']*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize frequent itemsets\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Support distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(frequent_itemsets['support'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Itemset Support', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Support')\n",
    "    plt.ylabel('Number of Itemsets')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Itemsets by length\n",
    "    plt.subplot(2, 2, 2)\n",
    "    itemset_length_counts.plot(kind='bar', alpha=0.8)\n",
    "    plt.title('Number of Itemsets by Length', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Itemset Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Support vs Length\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for length in sorted(frequent_itemsets['length'].unique()):\n",
    "        length_data = frequent_itemsets[frequent_itemsets['length'] == length]\n",
    "        plt.scatter([length] * len(length_data), length_data['support'], \n",
    "                   alpha=0.6, s=50, label=f'{length}-itemsets')\n",
    "    plt.title('Support vs Itemset Length', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Itemset Length')\n",
    "    plt.ylabel('Support')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Top itemsets bar chart\n",
    "    plt.subplot(2, 2, 4)\n",
    "    top_10 = frequent_itemsets.nlargest(10, 'support')\n",
    "    itemset_labels = [', '.join(list(itemsets)[:2]) + ('...' if len(itemsets) > 2 else '') \n",
    "                     for itemsets in top_10['itemsets']]\n",
    "    plt.barh(range(len(top_10)), top_10['support'], alpha=0.8)\n",
    "    plt.yticks(range(len(top_10)), itemset_labels)\n",
    "    plt.title('Top 10 Frequent Itemsets', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Support')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save frequent itemsets\n",
    "    frequent_itemsets_export = frequent_itemsets.copy()\n",
    "    frequent_itemsets_export['itemsets_str'] = frequent_itemsets_export['itemsets'].apply(\n",
    "        lambda x: ', '.join(list(x))\n",
    "    )\n",
    "    frequent_itemsets_export[['itemsets_str', 'support', 'length']].to_csv(\n",
    "        'frequent_itemsets.csv', index=False\n",
    "    )\n",
    "    print(f\"\\nüíæ Frequent itemsets saved to 'frequent_itemsets.csv'\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No frequent itemsets found. Consider reducing the minimum support threshold.\")\n",
    "\n",
    "print(\"\\n‚úÖ Frequent itemsets generation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc997864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE ASSOCIATION RULES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: GENERATING ASSOCIATION RULES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(frequent_itemsets) > 0:\n",
    "    # Generate association rules\n",
    "    min_confidence = 0.5  # 50% minimum confidence\n",
    "    print(f\"üéØ Minimum confidence threshold: {min_confidence} ({min_confidence*100}%)\")\n",
    "    \n",
    "    print(\"\\nüîç Generating association rules...\")\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    \n",
    "    if len(rules) == 0:\n",
    "        print(\"‚ö†Ô∏è No rules found with current confidence. Reducing threshold...\")\n",
    "        min_confidence = 0.3  # Reduce to 30%\n",
    "        rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "        \n",
    "        if len(rules) == 0:\n",
    "            print(\"‚ö†Ô∏è Still no rules. Using very low threshold...\")\n",
    "            min_confidence = 0.1  # Reduce to 10%\n",
    "            rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    \n",
    "    print(f\"\\nüìä Association Rules Analysis:\")\n",
    "    print(f\"   Total rules generated: {len(rules)}\")\n",
    "    print(f\"   Final minimum confidence used: {min_confidence} ({min_confidence*100}%)\")\n",
    "    \n",
    "    if len(rules) > 0:\n",
    "        # Calculate additional metrics\n",
    "        rules['leverage'] = rules['support'] - (rules['antecedent support'] * rules['consequent support'])\n",
    "        rules['conviction'] = (1 - rules['consequent support']) / (1 - rules['confidence'])\n",
    "        \n",
    "        # Sort by multiple criteria\n",
    "        rules_sorted = rules.sort_values(['confidence', 'lift', 'support'], ascending=False)\n",
    "        \n",
    "        # Display rule statistics\n",
    "        print(f\"\\nüìä Rule Statistics:\")\n",
    "        print(f\"   Average confidence: {rules['confidence'].mean():.3f}\")\n",
    "        print(f\"   Average lift: {rules['lift'].mean():.3f}\")\n",
    "        print(f\"   Average support: {rules['support'].mean():.3f}\")\n",
    "        print(f\"   Rules with lift > 1: {(rules['lift'] > 1).sum()} ({(rules['lift'] > 1).mean()*100:.1f}%)\")\n",
    "        \n",
    "        # Display top rules\n",
    "        print(f\"\\nüèÜ Top 20 Association Rules (by Confidence):\")\n",
    "        print(\"=\" * 120)\n",
    "        print(f\"{'Antecedent':<30} {'Consequent':<30} {'Supp':<6} {'Conf':<6} {'Lift':<6} {'Conv':<6}\")\n",
    "        print(\"=\" * 120)\n",
    "        \n",
    "        for idx, row in rules_sorted.head(20).iterrows():\n",
    "            antecedent = ', '.join(list(row['antecedents']))\n",
    "            consequent = ', '.join(list(row['consequents']))\n",
    "            antecedent = antecedent[:28] + '..' if len(antecedent) > 30 else antecedent\n",
    "            consequent = consequent[:28] + '..' if len(consequent) > 30 else consequent\n",
    "            \n",
    "            print(f\"{antecedent:<30} {consequent:<30} {row['support']:<6.3f} {row['confidence']:<6.3f} \"\n",
    "                  f\"{row['lift']:<6.2f} {row['conviction']:<6.2f}\")\n",
    "        \n",
    "        # Analyze rules by lift\n",
    "        high_lift_rules = rules[rules['lift'] > 1.5]\n",
    "        print(f\"\\nüöÄ High-Lift Rules (Lift > 1.5): {len(high_lift_rules)} rules\")\n",
    "        \n",
    "        if len(high_lift_rules) > 0:\n",
    "            print(\"\\nüéØ Top High-Lift Rules:\")\n",
    "            for idx, row in high_lift_rules.nlargest(10, 'lift').iterrows():\n",
    "                antecedent = ', '.join(list(row['antecedents']))\n",
    "                consequent = ', '.join(list(row['consequents']))\n",
    "                print(f\"   {antecedent} ‚Üí {consequent}\")\n",
    "                print(f\"      Support: {row['support']:.3f}, Confidence: {row['confidence']:.3f}, Lift: {row['lift']:.2f}\")\n",
    "        \n",
    "        # Visualize association rules\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Support vs Confidence scatter plot\n",
    "        plt.subplot(2, 3, 1)\n",
    "        scatter = plt.scatter(rules['support'], rules['confidence'], \n",
    "                            c=rules['lift'], cmap='viridis', alpha=0.6, s=50)\n",
    "        plt.colorbar(scatter, label='Lift')\n",
    "        plt.xlabel('Support')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.title('Support vs Confidence (colored by Lift)', fontsize=12, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Lift distribution\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.hist(rules['lift'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(x=1, color='red', linestyle='--', label='Lift = 1')\n",
    "        plt.xlabel('Lift')\n",
    "        plt.ylabel('Number of Rules')\n",
    "        plt.title('Distribution of Lift Values', fontsize=12, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confidence distribution\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.hist(rules['confidence'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Number of Rules')\n",
    "        plt.title('Distribution of Confidence Values', fontsize=12, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Support vs Lift\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.scatter(rules['support'], rules['lift'], alpha=0.6, s=50)\n",
    "        plt.axhline(y=1, color='red', linestyle='--', label='Lift = 1')\n",
    "        plt.xlabel('Support')\n",
    "        plt.ylabel('Lift')\n",
    "        plt.title('Support vs Lift', fontsize=12, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confidence vs Lift\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.scatter(rules['confidence'], rules['lift'], alpha=0.6, s=50)\n",
    "        plt.axhline(y=1, color='red', linestyle='--', label='Lift = 1')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Lift')\n",
    "        plt.title('Confidence vs Lift', fontsize=12, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Top rules by different metrics\n",
    "        plt.subplot(2, 3, 6)\n",
    "        top_by_lift = rules.nlargest(10, 'lift')\n",
    "        rule_labels = [f\"{', '.join(list(row['antecedents'])[:1])} ‚Üí {', '.join(list(row['consequents'])[:1])}\" \n",
    "                      for _, row in top_by_lift.iterrows()]\n",
    "        rule_labels = [label[:20] + '...' if len(label) > 20 else label for label in rule_labels]\n",
    "        plt.barh(range(len(top_by_lift)), top_by_lift['lift'], alpha=0.8)\n",
    "        plt.yticks(range(len(top_by_lift)), rule_labels)\n",
    "        plt.xlabel('Lift')\n",
    "        plt.title('Top 10 Rules by Lift', fontsize=12, fontweight='bold')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save association rules\n",
    "        rules_export = rules.copy()\n",
    "        rules_export['antecedents_str'] = rules_export['antecedents'].apply(lambda x: ', '.join(list(x)))\n",
    "        rules_export['consequents_str'] = rules_export['consequents'].apply(lambda x: ', '.join(list(x)))\n",
    "        \n",
    "        export_columns = ['antecedents_str', 'consequents_str', 'support', 'confidence', \n",
    "                         'lift', 'leverage', 'conviction']\n",
    "        rules_export[export_columns].to_csv('association_rules.csv', index=False)\n",
    "        print(f\"\\nüíæ Association rules saved to 'association_rules.csv'\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No association rules found. Consider reducing the confidence threshold.\")\n",
    "        rules = pd.DataFrame()  # Empty dataframe for later use\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot generate rules without frequent itemsets.\")\n",
    "    rules = pd.DataFrame()  # Empty dataframe for later use\n",
    "\n",
    "print(\"\\n‚úÖ Association rules generation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NETWORK VISUALIZATION OF ASSOCIATION RULES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: NETWORK VISUALIZATION OF ASSOCIATION RULES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(rules) > 0:\n",
    "    # Create network graph of top rules\n",
    "    print(\"\\nüï∏Ô∏è Creating network visualization...\")\n",
    "    \n",
    "    # Select top rules for visualization (to avoid cluttered graph)\n",
    "    top_rules_for_network = rules.nlargest(20, 'lift')\n",
    "    \n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    for idx, row in top_rules_for_network.iterrows():\n",
    "        antecedents = list(row['antecedents'])\n",
    "        consequents = list(row['consequents'])\n",
    "        \n",
    "        # Add nodes\n",
    "        for item in antecedents + consequents:\n",
    "            if not G.has_node(item):\n",
    "                G.add_node(item)\n",
    "        \n",
    "        # Add edges (for simplicity, connect first antecedent to first consequent)\n",
    "        if antecedents and consequents:\n",
    "            G.add_edge(antecedents[0], consequents[0], \n",
    "                      weight=row['lift'], \n",
    "                      confidence=row['confidence'],\n",
    "                      support=row['support'])\n",
    "    \n",
    "    # Create network visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Calculate layout\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_sizes = [300 + G.degree(node) * 100 for node in G.nodes()]\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, \n",
    "                          node_color='lightblue', alpha=0.7)\n",
    "    \n",
    "    # Draw edges with thickness based on lift\n",
    "    edges = G.edges(data=True)\n",
    "    edge_weights = [edge[2]['weight'] for edge in edges]\n",
    "    edge_widths = [min(max(weight * 2, 1), 5) for weight in edge_weights]\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, width=edge_widths, \n",
    "                          alpha=0.6, edge_color='gray', arrows=True, \n",
    "                          arrowsize=20, arrowstyle='->')\n",
    "    \n",
    "    # Draw labels\n",
    "    labels = {node: node.split('_')[-1][:8] for node in G.nodes()}  # Shortened labels\n",
    "    nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title('Association Rules Network\\n(Top 20 Rules by Lift)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "    plt.text(0.02, 0.98, 'Node size: degree centrality\\nEdge width: lift value', \n",
    "            transform=plt.gca().transAxes, fontsize=10, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Network statistics\n",
    "    print(f\"\\nüìä Network Statistics:\")\n",
    "    print(f\"   Nodes (products): {G.number_of_nodes()}\")\n",
    "    print(f\"   Edges (rules): {G.number_of_edges()}\")\n",
    "    print(f\"   Average degree: {2 * G.number_of_edges() / G.number_of_nodes():.2f}\")\n",
    "    \n",
    "    # Find most connected products\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    top_central_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    print(f\"\\nüîó Most Connected Products:\")\n",
    "    for node, centrality in top_central_nodes:\n",
    "        print(f\"   {node}: {centrality:.3f} (degree: {G.degree(node)})\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot create network visualization without association rules.\")\n",
    "\n",
    "print(\"\\n‚úÖ Network visualization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUSINESS INSIGHTS AND RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUSINESS INSIGHTS AND MARKET BASKET ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze product categories and associations\n",
    "print(f\"\\nüõí MARKET BASKET ANALYSIS INSIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if len(rules) > 0:\n",
    "    # Category-based analysis\n",
    "    print(f\"\\nüìä Rule Analysis by Product Categories:\")\n",
    "    \n",
    "    # Extract categories from product names\n",
    "    def extract_category(product_name):\n",
    "        return product_name.split('_')[0] if '_' in product_name else 'Unknown'\n",
    "    \n",
    "    # Analyze cross-category associations\n",
    "    cross_category_rules = []\n",
    "    within_category_rules = []\n",
    "    \n",
    "    for idx, row in rules.iterrows():\n",
    "        antecedent_categories = {extract_category(item) for item in row['antecedents']}\n",
    "        consequent_categories = {extract_category(item) for item in row['consequents']}\n",
    "        \n",
    "        if antecedent_categories.isdisjoint(consequent_categories):\n",
    "            cross_category_rules.append(row)\n",
    "        else:\n",
    "            within_category_rules.append(row)\n",
    "    \n",
    "    print(f\"\\nüîÑ Cross-Category vs Within-Category Rules:\")\n",
    "    print(f\"   Cross-category rules: {len(cross_category_rules)} ({len(cross_category_rules)/len(rules)*100:.1f}%)\")\n",
    "    print(f\"   Within-category rules: {len(within_category_rules)} ({len(within_category_rules)/len(rules)*100:.1f}%)\")\n",
    "    \n",
    "    # Top cross-category associations\n",
    "    if cross_category_rules:\n",
    "        cross_df = pd.DataFrame(cross_category_rules)\n",
    "        top_cross = cross_df.nlargest(10, 'lift')\n",
    "        \n",
    "        print(f\"\\nüîó Top Cross-Category Associations:\")\n",
    "        for idx, row in top_cross.iterrows():\n",
    "            antecedent = ', '.join(list(row['antecedents']))\n",
    "            consequent = ', '.join(list(row['consequents']))\n",
    "            print(f\"   {antecedent} ‚Üí {consequent}\")\n",
    "            print(f\"      Lift: {row['lift']:.2f}, Confidence: {row['confidence']:.3f}\")\n",
    "    \n",
    "    # Product recommendation analysis\n",
    "    print(f\"\\nüéØ PRODUCT RECOMMENDATION INSIGHTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find products that are frequently bought together\n",
    "    recommendation_dict = {}\n",
    "    \n",
    "    for idx, row in rules.nlargest(20, 'confidence').iterrows():\n",
    "        for antecedent in row['antecedents']:\n",
    "            if antecedent not in recommendation_dict:\n",
    "                recommendation_dict[antecedent] = []\n",
    "            \n",
    "            for consequent in row['consequents']:\n",
    "                recommendation_dict[antecedent].append({\n",
    "                    'product': consequent,\n",
    "                    'confidence': row['confidence'],\n",
    "                    'lift': row['lift']\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nüõçÔ∏è Product Recommendation Engine:\")\n",
    "    print(\"   If customer buys... they are likely to also buy:\")\n",
    "    \n",
    "    for product, recommendations in list(recommendation_dict.items())[:5]:\n",
    "        print(f\"\\n   üì¶ {product}:\")\n",
    "        # Sort recommendations by confidence\n",
    "        sorted_recs = sorted(recommendations, key=lambda x: x['confidence'], reverse=True)[:3]\n",
    "        for i, rec in enumerate(sorted_recs, 1):\n",
    "            print(f\"      {i}. {rec['product']} (confidence: {rec['confidence']:.3f}, lift: {rec['lift']:.2f})\")\n",
    "\n",
    "# Seasonal and temporal analysis (if timestamp data available)\n",
    "print(f\"\\nüìÖ STRATEGIC BUSINESS RECOMMENDATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "strategic_recommendations = [\n",
    "    \"üõí Store Layout Optimization:\",\n",
    "    \"   ‚Ä¢ Place frequently associated products near each other\",\n",
    "    \"   ‚Ä¢ Create product bundles based on high-lift associations\",\n",
    "    \"   ‚Ä¢ Position complementary items at end caps\",\n",
    "    \n",
    "    \"\\nüí∞ Pricing and Promotion Strategies:\",\n",
    "    \"   ‚Ä¢ Offer discounts on antecedent products to drive consequent sales\",\n",
    "    \"   ‚Ä¢ Create bundle pricing for strongly associated items\",\n",
    "    \"   ‚Ä¢ Use cross-selling promotions during peak shopping periods\",\n",
    "    \n",
    "    \"\\nüìß Personalized Marketing:\",\n",
    "    \"   ‚Ä¢ Send targeted recommendations based on purchase history\",\n",
    "    \"   ‚Ä¢ Create personalized email campaigns for product combinations\",\n",
    "    \"   ‚Ä¢ Develop customer-specific promotional offers\",\n",
    "    \n",
    "    \"\\nüì¶ Inventory Management:\",\n",
    "    \"   ‚Ä¢ Coordinate inventory levels for associated products\",\n",
    "    \"   ‚Ä¢ Predict demand for consequent items based on antecedent sales\",\n",
    "    \"   ‚Ä¢ Optimize supply chain for product bundles\",\n",
    "    \n",
    "    \"\\nüéØ Customer Segmentation:\",\n",
    "    \"   ‚Ä¢ Identify customer groups based on purchasing patterns\",\n",
    "    \"   ‚Ä¢ Develop targeted campaigns for different basket types\",\n",
    "    \"   ‚Ä¢ Create loyalty programs based on frequent associations\"\n",
    "]\n",
    "\n",
    "for recommendation in strategic_recommendations:\n",
    "    print(recommendation)\n",
    "\n",
    "# Performance metrics summary\n",
    "print(f\"\\nüìä ANALYSIS PERFORMANCE SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Total transactions analyzed: {len(transactions):,}\")\n",
    "print(f\"‚úÖ Unique products identified: {len(te.columns_):,}\")\n",
    "print(f\"‚úÖ Frequent itemsets found: {len(frequent_itemsets) if len(frequent_itemsets) > 0 else 0:,}\")\n",
    "print(f\"‚úÖ Association rules generated: {len(rules) if len(rules) > 0 else 0:,}\")\n",
    "print(f\"‚úÖ Minimum support threshold: {min_support} ({min_support*100:.1f}%)\")\n",
    "if len(rules) > 0:\n",
    "    print(f\"‚úÖ Average rule confidence: {rules['confidence'].mean():.3f}\")\n",
    "    print(f\"‚úÖ Average rule lift: {rules['lift'].mean():.2f}\")\n",
    "    print(f\"‚úÖ Strong rules (lift > 1): {(rules['lift'] > 1).sum():,} ({(rules['lift'] > 1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Export final summary\n",
    "analysis_summary = {\n",
    "    'transactions_count': len(transactions),\n",
    "    'unique_products': len(te.columns_) if 'te' in locals() else 0,\n",
    "    'frequent_itemsets_count': len(frequent_itemsets) if len(frequent_itemsets) > 0 else 0,\n",
    "    'association_rules_count': len(rules) if len(rules) > 0 else 0,\n",
    "    'min_support_used': min_support if 'min_support' in locals() else 0,\n",
    "    'avg_confidence': rules['confidence'].mean() if len(rules) > 0 else 0,\n",
    "    'avg_lift': rules['lift'].mean() if len(rules) > 0 else 0,\n",
    "    'strong_rules_count': (rules['lift'] > 1).sum() if len(rules) > 0 else 0\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('market_basket_analysis_summary.json', 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Analysis summary saved to 'market_basket_analysis_summary.json'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ MARKET BASKET ANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "print(\"Association rules identified and business insights generated\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
