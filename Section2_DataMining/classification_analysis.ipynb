{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd290230",
   "metadata": {},
   "source": [
    "# DSA 2040 Practical Exam - Section 2, Task 3\n",
    "## Classification - Customer Value Prediction\n",
    "\n",
    "**Student:** Monaheng218  \n",
    "**Date:** August 13, 2025  \n",
    "**Total Marks:** 10\n",
    "\n",
    "### Task Requirements:\n",
    "1. Build classification models to predict customer value category\n",
    "2. Compare multiple algorithms (Decision Tree, Random Forest, SVM, etc.)\n",
    "3. Evaluate model performance using appropriate metrics\n",
    "4. Feature importance analysis\n",
    "5. Provide business insights for customer value prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Classification libraries imported successfully\")\n",
    "print(\"üéØ Ready for customer value prediction analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the preprocessed classification dataset\n",
    "X_classification = pd.read_csv('classification_features.csv')\n",
    "y_classification = pd.read_csv('classification_target.csv')['CustomerValueCategory']\n",
    "df_classification_full = pd.read_csv('classification_dataset_full.csv')\n",
    "\n",
    "print(f\"üìä Classification dataset loaded successfully\")\n",
    "print(f\"Features shape: {X_classification.shape}\")\n",
    "print(f\"Target shape: {y_classification.shape}\")\n",
    "print(f\"Features: {list(X_classification.columns)}\")\n",
    "\n",
    "# Check target distribution\n",
    "target_dist = pd.Series(y_classification).value_counts()\n",
    "print(f\"\\nüéØ Target Variable Distribution:\")\n",
    "for class_label, count in target_dist.items():\n",
    "    percentage = (count / len(y_classification)) * 100\n",
    "    print(f\"   Class {class_label}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "target_dist.plot(kind='bar', alpha=0.8)\n",
    "plt.title('Customer Value Category Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Customer Value Category')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(target_dist.values, labels=target_dist.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Customer Value Category Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nüìä Feature Statistics:\")\n",
    "print(X_classification.describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = X_classification.isnull().sum().sum()\n",
    "print(f\"\\n‚ùå Missing values: {missing_values}\")\n",
    "\n",
    "if missing_values == 0:\n",
    "    print(\"‚úÖ Data is clean and ready for classification\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Handling missing values...\")\n",
    "    X_classification = X_classification.fillna(X_classification.mean())\n",
    "    print(\"‚úÖ Missing values handled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA SPLITTING AND PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: DATA SPLITTING AND PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_classification, y_classification, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_classification\n",
    ")\n",
    "\n",
    "print(f\"üìä Data splitting completed:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"   Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Check class distribution in train/test sets\n",
    "print(f\"\\nüéØ Class distribution in training set:\")\n",
    "train_dist = pd.Series(y_train).value_counts()\n",
    "for class_label, count in train_dist.items():\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"   Class {class_label}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Class distribution in testing set:\")\n",
    "test_dist = pd.Series(y_test).value_counts()\n",
    "for class_label, count in test_dist.items():\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    print(f\"   Class {class_label}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Feature correlation analysis\n",
    "print(f\"\\nüîç Feature Correlation Analysis:\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = X_train.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "           square=True, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"‚ö†Ô∏è High correlation pairs found:\")\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"   {feat1} - {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(f\"‚úÖ No highly correlated features found (threshold: 0.8)\")\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610492c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL TRAINING AND COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: MODEL TRAINING AND COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define classification models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "model_objects = {}\n",
    "\n",
    "print(\"\\nüîß Training and evaluating models...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n   üìä Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    model_objects[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'CV Mean': cv_mean,\n",
    "        'CV Std': cv_std,\n",
    "        'Predictions': y_pred,\n",
    "        'Probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"      ‚úÖ Accuracy: {accuracy:.3f}, F1-Score: {f1:.3f}, CV: {cv_mean:.3f}¬±{cv_std:.3f}\")\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    model: {\n",
    "        'Accuracy': metrics['Accuracy'],\n",
    "        'Precision': metrics['Precision'],\n",
    "        'Recall': metrics['Recall'],\n",
    "        'F1-Score': metrics['F1-Score'],\n",
    "        'CV Mean': metrics['CV Mean'],\n",
    "        'CV Std': metrics['CV Std']\n",
    "    }\n",
    "    for model, metrics in results.items()\n",
    "}).T\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüìä MODEL COMPARISON RESULTS:\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df['F1-Score'].idxmax()\n",
    "best_model = model_objects[best_model_name]\n",
    "best_f1 = results_df.loc[best_model_name, 'F1-Score']\n",
    "\n",
    "print(f\"\\nüèÜ Best performing model: {best_model_name}\")\n",
    "print(f\"   üéØ F1-Score: {best_f1:.3f}\")\n",
    "print(f\"   üìä Cross-validation: {results_df.loc[best_model_name, 'CV Mean']:.3f}¬±{results_df.loc[best_model_name, 'CV Std']:.3f}\")\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Performance metrics comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(results_df.index))\n",
    "width = 0.2\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.bar(x + i * width, results_df[metric], width, label=metric, alpha=0.8)\n",
    "\n",
    "plt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(x + width * 1.5, results_df.index, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Cross-validation scores\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.errorbar(range(len(results_df)), results_df['CV Mean'], \n",
    "            yerr=results_df['CV Std'], marker='o', capsize=5, linewidth=2)\n",
    "plt.title('Cross-Validation Scores with Error Bars', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('CV Accuracy')\n",
    "plt.xticks(range(len(results_df)), results_df.index, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score ranking\n",
    "plt.subplot(2, 2, 3)\n",
    "f1_sorted = results_df.sort_values('F1-Score', ascending=True)\n",
    "plt.barh(range(len(f1_sorted)), f1_sorted['F1-Score'], alpha=0.8)\n",
    "plt.title('F1-Score Ranking', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('F1-Score')\n",
    "plt.yticks(range(len(f1_sorted)), f1_sorted.index)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy vs F1-Score scatter\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(results_df['Accuracy'], results_df['F1-Score'], s=100, alpha=0.7)\n",
    "for i, model in enumerate(results_df.index):\n",
    "    plt.annotate(model, (results_df['Accuracy'].iloc[i], results_df['F1-Score'].iloc[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "plt.title('Accuracy vs F1-Score', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Model training and comparison completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETAILED EVALUATION OF BEST MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: DETAILED EVALUATION OF BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions from best model\n",
    "best_predictions = results[best_model_name]['Predictions']\n",
    "best_probabilities = results[best_model_name]['Probabilities']\n",
    "\n",
    "print(f\"\\nüîç Detailed evaluation of {best_model_name}:\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "class_names = sorted(y_test.unique())\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Confusion Matrix (Normalized)\n",
    "plt.subplot(1, 3, 2)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Normalized Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# ROC Curves (for multi-class)\n",
    "plt.subplot(1, 3, 3)\n",
    "if best_probabilities is not None:\n",
    "    # Convert to binary format for ROC analysis\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from itertools import cycle\n",
    "    \n",
    "    # Binarize the output\n",
    "    y_test_bin = label_binarize(y_test, classes=class_names)\n",
    "    n_classes = y_test_bin.shape[1]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], best_probabilities[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, linewidth=2,\n",
    "                label=f'Class {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'ROC curve not available\\nfor this model', \n",
    "            ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title('ROC Curve - Not Available', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class metrics\n",
    "print(f\"\\nüìä Per-Class Performance:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = y_test == class_name\n",
    "    class_predictions = best_predictions == class_name\n",
    "    \n",
    "    tp = np.sum(class_mask & class_predictions)\n",
    "    fp = np.sum(~class_mask & class_predictions)\n",
    "    tn = np.sum(~class_mask & ~class_predictions)\n",
    "    fn = np.sum(class_mask & ~class_predictions)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"   Class {class_name}:\")\n",
    "    print(f\"      Precision: {precision:.3f}\")\n",
    "    print(f\"      Recall: {recall:.3f}\")\n",
    "    print(f\"      F1-Score: {f1:.3f}\")\n",
    "    print(f\"      Support: {np.sum(class_mask)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Detailed evaluation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract feature importance from best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Feature Importance ({best_model_name}):\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(range(len(importance_df)), importance_df['Importance'], alpha=0.8)\n",
    "    plt.yticks(range(len(importance_df)), importance_df['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    cumulative_importance = importance_df['Importance'].cumsum()\n",
    "    plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, \n",
    "            marker='o', linewidth=2, markersize=6)\n",
    "    plt.axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "    plt.axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Cumulative Importance')\n",
    "    plt.title('Cumulative Feature Importance', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find features contributing to 80% and 90% of importance\n",
    "    importance_80 = cumulative_importance <= 0.8\n",
    "    importance_90 = cumulative_importance <= 0.9\n",
    "    \n",
    "    features_80 = importance_df[importance_80]['Feature'].tolist()\n",
    "    features_90 = importance_df[importance_90]['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nüéØ Key Feature Insights:\")\n",
    "    print(f\"   üìà Top 3 most important features:\")\n",
    "    for i, (_, row) in enumerate(importance_df.head(3).iterrows()):\n",
    "        print(f\"      {i+1}. {row['Feature']}: {row['Importance']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n   üìä Features contributing to 80% of importance ({len(features_80)} features):\")\n",
    "    print(f\"      {', '.join(features_80)}\")\n",
    "    \n",
    "    print(f\"\\n   üìä Features contributing to 90% of importance ({len(features_90)} features):\")\n",
    "    print(f\"      {', '.join(features_90)}\")\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models\n",
    "    coef_importance = np.abs(best_model.coef_[0]) if best_model.coef_.ndim > 1 else np.abs(best_model.coef_)\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': coef_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Feature Importance (Coefficient Magnitude - {best_model_name}):\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(importance_df)), importance_df['Importance'], alpha=0.8)\n",
    "    plt.yticks(range(len(importance_df)), importance_df['Feature'])\n",
    "    plt.xlabel('Feature Importance (|Coefficient|)')\n",
    "    plt.title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Feature importance not available for {best_model_name}\")\n",
    "    \n",
    "    # Use permutation importance as alternative\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    print(\"\\nüîç Computing permutation importance...\")\n",
    "    perm_importance = permutation_importance(best_model, X_test, y_test, \n",
    "                                           n_repeats=10, random_state=42)\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': perm_importance.importances_mean,\n",
    "        'Std': perm_importance.importances_std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Permutation Feature Importance:\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    # Visualize permutation importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(importance_df)), importance_df['Importance'], \n",
    "            xerr=importance_df['Std'], alpha=0.8)\n",
    "    plt.yticks(range(len(importance_df)), importance_df['Feature'])\n",
    "    plt.xlabel('Permutation Importance')\n",
    "    plt.title(f'Permutation Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature importance analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e802333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL OPTIMIZATION (HYPERPARAMETER TUNING)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: MODEL OPTIMIZATION (HYPERPARAMETER TUNING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define parameter grids for top models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select top 2 models for optimization\n",
    "top_models = results_df.nlargest(2, 'F1-Score').index.tolist()\n",
    "print(f\"\\nüéØ Optimizing top 2 models: {', '.join(top_models)}\")\n",
    "\n",
    "optimized_results = {}\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in param_grids:\n",
    "        print(f\"\\nüîß Optimizing {model_name}...\")\n",
    "        \n",
    "        # Get the base model\n",
    "        base_model = model_objects[model_name]\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            base_model, \n",
    "            param_grids[model_name], \n",
    "            cv=5, \n",
    "            scoring='f1_weighted', \n",
    "            n_jobs=-1, \n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get optimized model\n",
    "        optimized_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate optimized model\n",
    "        y_pred_opt = optimized_model.predict(X_test)\n",
    "        \n",
    "        opt_accuracy = accuracy_score(y_test, y_pred_opt)\n",
    "        opt_f1 = f1_score(y_test, y_pred_opt, average='weighted')\n",
    "        opt_cv_scores = cross_val_score(optimized_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        \n",
    "        optimized_results[model_name] = {\n",
    "            'Best Params': grid_search.best_params_,\n",
    "            'Best CV Score': grid_search.best_score_,\n",
    "            'Test Accuracy': opt_accuracy,\n",
    "            'Test F1': opt_f1,\n",
    "            'CV Mean': opt_cv_scores.mean(),\n",
    "            'CV Std': opt_cv_scores.std(),\n",
    "            'Model': optimized_model\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"   üìä Best CV score: {grid_search.best_score_:.3f}\")\n",
    "        print(f\"   üéØ Test F1-score: {opt_f1:.3f}\")\n",
    "        \n",
    "        # Compare with original model\n",
    "        original_f1 = results[model_name]['F1-Score']\n",
    "        improvement = opt_f1 - original_f1\n",
    "        print(f\"   üìà Improvement over original: {improvement:+.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No parameter grid defined for {model_name}, skipping optimization\")\n",
    "\n",
    "# Display optimization results\n",
    "if optimized_results:\n",
    "    print(f\"\\nüìä OPTIMIZATION RESULTS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    opt_df = pd.DataFrame({\n",
    "        model: {\n",
    "            'Original F1': results[model]['F1-Score'],\n",
    "            'Optimized F1': metrics['Test F1'],\n",
    "            'Improvement': metrics['Test F1'] - results[model]['F1-Score'],\n",
    "            'CV Score': metrics['Best CV Score']\n",
    "        }\n",
    "        for model, metrics in optimized_results.items()\n",
    "    }).T\n",
    "    \n",
    "    print(opt_df.round(3))\n",
    "    \n",
    "    # Find best optimized model\n",
    "    best_opt_model_name = opt_df['Optimized F1'].idxmax()\n",
    "    best_opt_f1 = opt_df.loc[best_opt_model_name, 'Optimized F1']\n",
    "    \n",
    "    print(f\"\\nüèÜ Best optimized model: {best_opt_model_name}\")\n",
    "    print(f\"   üéØ Optimized F1-Score: {best_opt_f1:.3f}\")\n",
    "    print(f\"   üìà Total improvement: {opt_df.loc[best_opt_model_name, 'Improvement']:+.3f}\")\n",
    "    \n",
    "    # Update best model if optimization improved performance\n",
    "    if best_opt_f1 > best_f1:\n",
    "        best_model = optimized_results[best_opt_model_name]['Model']\n",
    "        best_model_name = f\"{best_opt_model_name} (Optimized)\"\n",
    "        print(f\"\\n‚úÖ Updated best model to optimized version\")\n",
    "    else:\n",
    "        print(f\"\\nüìä Original model performs better, keeping original\")\n",
    "\n",
    "print(\"\\n‚úÖ Model optimization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUSINESS INSIGHTS AND RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUSINESS INSIGHTS AND CUSTOMER VALUE PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate predictions for the full dataset\n",
    "full_predictions = best_model.predict(X_classification)\n",
    "full_probabilities = best_model.predict_proba(X_classification) if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# Add predictions to the original dataset\n",
    "df_classification_full['PredictedValue'] = full_predictions\n",
    "if full_probabilities is not None:\n",
    "    for i, class_name in enumerate(sorted(y_classification.unique())):\n",
    "        df_classification_full[f'Prob_{class_name}'] = full_probabilities[:, i]\n",
    "\n",
    "# Analyze prediction accuracy by segment\n",
    "print(f\"\\nüéØ PREDICTION ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall accuracy on full dataset\n",
    "full_accuracy = accuracy_score(y_classification, full_predictions)\n",
    "print(f\"üìä Overall prediction accuracy: {full_accuracy:.3f}\")\n",
    "\n",
    "# Prediction distribution\n",
    "pred_dist = pd.Series(full_predictions).value_counts()\n",
    "actual_dist = pd.Series(y_classification).value_counts()\n",
    "\n",
    "print(f\"\\nüìä Prediction vs Actual Distribution:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': actual_dist,\n",
    "    'Predicted': pred_dist\n",
    "}).fillna(0)\n",
    "comparison_df['Difference'] = comparison_df['Predicted'] - comparison_df['Actual']\n",
    "comparison_df['Difference_Pct'] = (comparison_df['Difference'] / comparison_df['Actual']) * 100\n",
    "print(comparison_df)\n",
    "\n",
    "# Model confidence analysis\n",
    "if full_probabilities is not None:\n",
    "    # Calculate prediction confidence (max probability)\n",
    "    prediction_confidence = np.max(full_probabilities, axis=1)\n",
    "    df_classification_full['Confidence'] = prediction_confidence\n",
    "    \n",
    "    print(f\"\\nüîç Prediction Confidence Analysis:\")\n",
    "    print(f\"   Average confidence: {prediction_confidence.mean():.3f}\")\n",
    "    print(f\"   Confidence std: {prediction_confidence.std():.3f}\")\n",
    "    print(f\"   High confidence predictions (>0.8): {(prediction_confidence > 0.8).sum()} ({(prediction_confidence > 0.8).mean()*100:.1f}%)\")\n",
    "    print(f\"   Low confidence predictions (<0.6): {(prediction_confidence < 0.6).sum()} ({(prediction_confidence < 0.6).mean()*100:.1f}%)\")\n",
    "\n",
    "# Business insights based on feature importance and predictions\n",
    "print(f\"\\nüí° BUSINESS INSIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze which features are most predictive\n",
    "print(f\"\\nüîç Key Factors for Customer Value Prediction:\")\n",
    "if 'importance_df' in locals():\n",
    "    top_features = importance_df.head(3)\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        feature_name = row['Feature']\n",
    "        importance = row['Importance']\n",
    "        \n",
    "        print(f\"\\n{i}. {feature_name} (Importance: {importance:.3f})\")\n",
    "        \n",
    "        # Analyze feature by customer value category\n",
    "        feature_analysis = df_classification_full.groupby('CustomerValueCategory')[feature_name].agg(['mean', 'std']).round(3)\n",
    "        \n",
    "        for category in feature_analysis.index:\n",
    "            mean_val = feature_analysis.loc[category, 'mean']\n",
    "            std_val = feature_analysis.loc[category, 'std']\n",
    "            print(f\"   {category} customers: {mean_val:.3f} ¬± {std_val:.3f}\")\n",
    "\n",
    "# Customer value transition analysis\n",
    "print(f\"\\nüìà CUSTOMER VALUE INSIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze misclassifications to understand edge cases\n",
    "test_indices = X_test.index\n",
    "misclassified_mask = y_test != best_predictions\n",
    "misclassified_indices = test_indices[misclassified_mask]\n",
    "\n",
    "if len(misclassified_indices) > 0:\n",
    "    print(f\"\\nüîç Misclassification Analysis ({len(misclassified_indices)} cases):\")\n",
    "    \n",
    "    misclass_analysis = pd.DataFrame({\n",
    "        'Actual': y_test[misclassified_mask],\n",
    "        'Predicted': best_predictions[misclassified_mask]\n",
    "    })\n",
    "    \n",
    "    common_errors = misclass_analysis.groupby(['Actual', 'Predicted']).size().sort_values(ascending=False)\n",
    "    print(\"   Most common misclassification patterns:\")\n",
    "    for (actual, predicted), count in common_errors.head(5).items():\n",
    "        print(f\"   ‚Ä¢ {actual} ‚Üí {predicted}: {count} cases\")\n",
    "\n",
    "# Strategic recommendations\n",
    "print(f\"\\nüöÄ STRATEGIC RECOMMENDATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "recommendations = [\n",
    "    \"üìä Use the model to predict customer value early in the relationship\",\n",
    "    \"üéØ Focus marketing efforts on high-potential customers identified by the model\",\n",
    "    \"üìà Monitor key predictive features to identify value improvement opportunities\",\n",
    "    \"üîÑ Regularly retrain the model with new customer data to maintain accuracy\",\n",
    "    \"üí° Implement targeted interventions for customers predicted to be at risk of low value\",\n",
    "    \"üìß Personalize customer communications based on predicted value category\",\n",
    "    \"üéÅ Design loyalty programs that align with predicted customer value segments\"\n",
    "]\n",
    "\n",
    "for i, recommendation in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {recommendation}\")\n",
    "\n",
    "# Model deployment considerations\n",
    "print(f\"\\n‚öôÔ∏è MODEL DEPLOYMENT CONSIDERATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Model Type: {best_model_name}\")\n",
    "print(f\"‚úÖ Accuracy: {results_df.loc[best_model_name.replace(' (Optimized)', ''), 'Accuracy']:.3f}\")\n",
    "print(f\"‚úÖ F1-Score: {best_f1:.3f}\")\n",
    "print(f\"‚úÖ Cross-validation stability: ¬±{results_df.loc[best_model_name.replace(' (Optimized)', ''), 'CV Std']:.3f}\")\n",
    "print(f\"‚úÖ Feature requirements: {len(X_classification.columns)} features\")\n",
    "print(f\"‚úÖ Training data size: {len(X_train)} samples\")\n",
    "\n",
    "# Save final model and results\n",
    "import pickle\n",
    "\n",
    "# Save the best model\n",
    "with open('best_classification_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save predictions\n",
    "df_classification_full.to_csv('customer_value_predictions.csv', index=False)\n",
    "\n",
    "# Save model performance metrics\n",
    "model_summary = {\n",
    "    'best_model_name': best_model_name,\n",
    "    'accuracy': full_accuracy,\n",
    "    'f1_score': best_f1,\n",
    "    'feature_importance': importance_df.to_dict('records') if 'importance_df' in locals() else None,\n",
    "    'class_distribution': pred_dist.to_dict()\n",
    "}\n",
    "\n",
    "with open('classification_summary.pkl', 'wb') as f:\n",
    "    pickle.dump(model_summary, f)\n",
    "\n",
    "print(f\"\\nüíæ Model and results saved:\")\n",
    "print(f\"   ‚Ä¢ best_classification_model.pkl\")\n",
    "print(f\"   ‚Ä¢ customer_value_predictions.csv\")\n",
    "print(f\"   ‚Ä¢ classification_summary.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ CUSTOMER VALUE CLASSIFICATION COMPLETED SUCCESSFULLY\")\n",
    "print(\"Model trained, evaluated, and ready for deployment\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
